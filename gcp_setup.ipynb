{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install all the required modules/packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP dependencies (GCS, GCS file system, Vertex AI SDK) and Kubeflow Pipelines SDK.\n",
    "# !pip install google-cloud-storage gcsfs google-cloud-aiplatform kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup GCP-related global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = ''\n",
    "REGION = ''\n",
    "BUCKET_NAME = ''\n",
    "GCS_BUCKET_URI = f'gs://{BUCKET_NAME}'\n",
    "PIPELINE_ROOT = f'{GCS_BUCKET_URI}/pipeline_root/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload data to the Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication with service account key.\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCP Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_gcs(bucket_name, detination_blob_name, source_file_path):\n",
    "  # Bucket should exist. Folder/Directory structure in the destination_blob_name can be managed/auto-created.\n",
    "\n",
    "  client = storage.Client()\n",
    "  bucket = client.bucket(bucket_name)\n",
    "  blob = bucket.blob(detination_blob_name)\n",
    "  blob.upload_from_filename(source_file_path)\n",
    "\n",
    "  print('Upload Successful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_to_gcs(BUCKET_NAME, 'data/data.csv', 'data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_to_gcs(BUCKET_NAME, 'component_scripts/data_preprocessing.py', 'component_scripts/data_preprocessing.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas & GCSFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('data/product_details.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directly from a Pandas DataFrame.\n",
    "df.to_excel(f'{GCS_BUCKET_URI}/data/product_details.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define the Pipleine Components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "kfp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFP SDK Domain-Specific Language.\n",
    "from kfp.dsl import component, Input, Output, Dataset, Model, Metrics, ClassificationMetrics\n",
    "\n",
    "@component(\n",
    "    packages_to_install = ['google-cloud-storage', 'pandas', 'gcsfs', 'openpyxl'],\n",
    "    base_image = 'python',\n",
    "    output_component_file = 'data_preprocessing.yaml'\n",
    ")\n",
    "def data_preprocessing(project_id: str, bucket_name: str, script_path: str, data_gcs_uri: str, product_details_gcs_uri: str, processed_data: Output[Dataset], metrics: Output[Metrics]):\n",
    "    from google.cloud import storage\n",
    "    import importlib.util\n",
    "\n",
    "    # Downloading the component script from GCS.\n",
    "    client = storage.Client(project = project_id)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(script_path)\n",
    "    blob.download_to_filename('data_preprocessing.py')\n",
    "\n",
    "    spec = importlib.util.spec_from_file_location('data_preprocessing', 'data_preprocessing.py')\n",
    "    data_preprocessing_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(data_preprocessing_module)\n",
    "\n",
    "    df = data_preprocessing_module.data_preprocessing(data_gcs_uri, product_details_gcs_uri) # DataFrame.\n",
    "\n",
    "    # Output Artifacts (Dataset & Metrics).\n",
    "    df.to_csv(processed_data.path + '.csv', index = False)\n",
    "    processed_data.metadata['shape'] = f'{df.shape}'\n",
    "    metrics.log_metric('num_of_samples', df.shape[0])\n",
    "    metrics.log_metric('num_of_features', df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall RFM metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = ['pandas'],\n",
    "    base_image = 'python'\n",
    ")\n",
    "def overall_rfm_metrics(processed_data: Input[Dataset], rfm: Output[Dataset], metrics: Output[Metrics]):\n",
    "    import pandas as pd\n",
    "\n",
    "    # Input Artifact (Dataset).\n",
    "    df = pd.read_csv(processed_data.path + '.csv')\n",
    "\n",
    "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "\n",
    "    # RFM calculation.\n",
    "    reference_date = df['InvoiceDate'].max()\n",
    "    _rfm = df.groupby('CustomerID').agg(\n",
    "        Recency = ('InvoiceDate', lambda s: (reference_date - s.max()).days),\n",
    "        Frequency = ('InvoiceNo', 'nunique'),\n",
    "        Monetary = ('TotalPrice', 'sum')\n",
    "    ).copy()\n",
    "    _rfm.reset_index(inplace = True)\n",
    "\n",
    "    # Output Artifacts (Dataset & Metrics).\n",
    "    _rfm.to_csv(rfm.path + '.csv', index = False)\n",
    "    rfm.metadata['shape'] = f'{_rfm.shape}'\n",
    "    metrics.log_metric('num_of_samples', _rfm.shape[0])\n",
    "    metrics.log_metric('num_of_features', _rfm.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFM scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = ['pandas', 'scikit-learn'],\n",
    "    base_image = 'python'\n",
    ")\n",
    "def rfm_scaling(rfm: Input[Dataset], rfm_scaled: Output[Dataset]):\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # Input Artifact (Dataset).\n",
    "    df = pd.read_csv(rfm.path + '.csv')\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    _rfm_scaled = scaler.fit_transform(df.drop('CustomerID', axis = 1))\n",
    "    _rfm_scaled = pd.DataFrame(_rfm_scaled, columns = ['Recency', 'Frequency', 'Monetary'])\n",
    "    _rfm_scaled['CustomerID'] = df['CustomerID']\n",
    "\n",
    "    # Output Artifact (Dataset).\n",
    "    _rfm_scaled.to_csv(rfm_scaled.path + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = ['pandas', 'scikit-learn', 'joblib'],\n",
    "    base_image = 'python'\n",
    ")\n",
    "def kmeans_training_and_prediction(rfm_scaled: Input[Dataset], n_clusters: int, model: Output[Model], predictions: Output[Dataset], metrics: Output[Metrics]):\n",
    "    import pandas as pd\n",
    "    from sklearn.cluster import KMeans\n",
    "    from joblib import dump\n",
    "\n",
    "    # Input Artifact (Dataset).\n",
    "    df = pd.read_csv(rfm_scaled.path + '.csv')\n",
    "\n",
    "    _predictions = pd.DataFrame([], columns = ['CustomerID', 'Cluster'])\n",
    "    _predictions['CustomerID'] = df['CustomerID']\n",
    "    kmeans = KMeans(\n",
    "        n_clusters = n_clusters,\n",
    "        random_state = 123\n",
    "    )\n",
    "    _predictions['Cluster'] = kmeans.fit_predict(df.drop('CustomerID', axis = 1))\n",
    "\n",
    "    # Output Artifact (Model, Dataset, and Metrics).\n",
    "    dump(kmeans, model.path + '.joblib') # 'model.joblib/pkl' name is mandatory for deployment.\n",
    "    model.metadata['framework'] = 'scikit_learn'\n",
    "    model.metadata['algorithm'] = 'kmeans'    \n",
    "    _predictions.to_csv(predictions.path + '.csv', index = False)\n",
    "    metrics.log_metric('inertia', kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer Churn data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = ['pandas'],\n",
    "    base_image = 'python'\n",
    ")\n",
    "def customer_churn_data(processed_data: Input[Dataset], rfm_churn: Output[Dataset], metrics: Output[Metrics]):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import datetime\n",
    "\n",
    "    # Input Artifact (Dataset).\n",
    "    df = pd.read_csv(processed_data.path + '.csv')\n",
    "\n",
    "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "\n",
    "    # Dataset creation.\n",
    "    min_reference_date = df['InvoiceDate'].min() + datetime.timedelta(days = 15)\n",
    "    max_reference_date = df['InvoiceDate'].max() - datetime.timedelta(days = 60)\n",
    "\n",
    "    np.random.seed(123)\n",
    "    # 30 random dates between the min and max reference date.\n",
    "    reference_dates = [min_reference_date + (max_reference_date - min_reference_date) * np.random.random() for _ in range(30)]\n",
    "\n",
    "    _customer_churn_data = []\n",
    "\n",
    "    for reference_date in reference_dates:\n",
    "        rfm = df[df['InvoiceDate'] <= reference_date].groupby('CustomerID').agg(\n",
    "            Recency = ('InvoiceDate', lambda s: (reference_date - s.max()).days),\n",
    "            Frequency = ('InvoiceNo', 'nunique'),\n",
    "            Monetary = ('TotalPrice', 'sum')\n",
    "        )\n",
    "        \n",
    "        _max_reference_date = reference_date + datetime.timedelta(days = 60)\n",
    "        churn = pd.Series(index = rfm.index, data = 1, name = 'Churn')\n",
    "        returning_customers = set(df[(df['InvoiceDate'] > reference_date) & (df['InvoiceDate'] <= _max_reference_date)]['CustomerID'])\n",
    "        churn.loc[list(set(churn.index).intersection(returning_customers))] = 0\n",
    "        \n",
    "        rfm = pd.concat([\n",
    "            rfm,\n",
    "            churn], axis = 1)\n",
    "        rfm['Quarter'] = reference_date.quarter\n",
    "        \n",
    "        _customer_churn_data.append(rfm)\n",
    "\n",
    "    _customer_churn_data = pd.concat(_customer_churn_data, axis = 0, ignore_index = True)\n",
    "\n",
    "    # Output Artifacts (Dataset & Metrics).\n",
    "    _customer_churn_data.to_csv(rfm_churn.path + '.csv', index = False)\n",
    "    rfm_churn.metadata['shape'] = f'{_customer_churn_data.shape}'\n",
    "    metrics.log_metric('num_of_samples', _customer_churn_data.shape[0])\n",
    "    metrics.log_metric('num_of_features', _customer_churn_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = ['pandas', 'scikit-learn'],\n",
    "    base_image = 'python'\n",
    ")\n",
    "def train_test_split(rfm_churn: Input[Dataset], train_data: Output[Dataset], test_data: Output[Dataset], metrics: Output[Metrics]):\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Input Artifact (Dataset).\n",
    "    df = pd.read_csv(rfm_churn.path + '.csv')\n",
    "\n",
    "    _train_data, _test_data = train_test_split(df, test_size = .3, random_state = 123)\n",
    "\n",
    "    # Output Artifacts (Datasets & Metrics).\n",
    "    _train_data.to_csv(train_data.path + '.csv', index = False)\n",
    "    _test_data.to_csv(test_data.path + '.csv', index = False)\n",
    "    metrics.log_metric('train_data_samples', _train_data.shape[0])\n",
    "    metrics.log_metric('test_data_samples', _test_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFC (Random Forest Classifier) training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = ['pandas', 'scikit-learn', 'joblib'],\n",
    "    base_image = 'python'\n",
    ")\n",
    "def rfc_training(train_data: Input[Dataset], hyperparams: dict, model: Output[Model]):\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from joblib import dump\n",
    "\n",
    "    # Input Artifact (Dataset).\n",
    "    df = pd.read_csv(train_data.path + '.csv')\n",
    "    X_train, y_train = df.drop('Churn', axis = 1), df['Churn']\n",
    "\n",
    "    rfc = RandomForestClassifier(\n",
    "        n_estimators = hyperparams['n_estimators'],\n",
    "        min_samples_split = hyperparams['min_samples_split'],\n",
    "        min_samples_leaf = hyperparams['min_samples_leaf'],\n",
    "        random_state = 123\n",
    "    )\n",
    "    rfc.fit(X_train, y_train)\n",
    "\n",
    "    # Output Artifact (Model).\n",
    "    dump(rfc, model.path + '.joblib')\n",
    "    model.metadata['framework'] = 'scikit_learn'\n",
    "    model.metadata['algorithm'] = 'random_forest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFC (Random Forest Classifier) evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "@component(\n",
    "    packages_to_install = ['joblib', 'pandas', 'scikit-learn'],\n",
    "    base_image = 'python'\n",
    ")\n",
    "def rfc_evaluation(model: Input[Model], test_data: Input[Dataset], classification_metrics: Output[ClassificationMetrics], metrics: Output[Metrics], baseline_precision: float) -> NamedTuple('output', [('deployment', str)]):\n",
    "    from joblib import load\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_recall_curve, auc, confusion_matrix, roc_curve\n",
    "\n",
    "    # Input Artifacts (Model & Dataset).\n",
    "    rfc = load(model.path + '.joblib')\n",
    "    df = pd.read_csv(test_data.path + '.csv')\n",
    "\n",
    "    X_test, y_test = df.drop('Churn', axis = 1), df['Churn']\n",
    "    y_predicted_proba = rfc.predict_proba(X_test)\n",
    "    y_predicted = y_predicted_proba[:, 1].copy()\n",
    "    y_predicted[y_predicted >= 0.5] = 1 # Broadcasting.\n",
    "    y_predicted[y_predicted < 0.5] = 0 # Broadcasting.\n",
    "\n",
    "    # Output Artifacts (Metrics & Classification Metrics).\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    metrics.log_metric('accuracy', accuracy)\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_predicted)\n",
    "    metrics.log_metric('precision:0', precision[0])\n",
    "    metrics.log_metric('recall:0', recall[0])\n",
    "    metrics.log_metric('f1_score:0', f1_score[0])\n",
    "    metrics.log_metric('precision:1', precision[1])\n",
    "    model_precision = precision[1]\n",
    "    metrics.log_metric('recall:1', recall[1])\n",
    "    metrics.log_metric('f1_score:1', f1_score[1])\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_predicted_proba[:, 1])\n",
    "    metrics.log_metric('auc_precision_recall', auc(recall, precision))\n",
    "\n",
    "    classification_metrics.log_confusion_matrix(['0', '1'], confusion_matrix(y_test, y_predicted).tolist())\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_predicted_proba[:, 1])\n",
    "    metrics.log_metric('auc_roc', auc(fpr, tpr))\n",
    "    # Sampled FPR & TPR values (executor_output.json size restrictions).\n",
    "    roc = pd.DataFrame({\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'thresholds': thresholds\n",
    "    })\n",
    "    roc = pd.concat([\n",
    "        roc[roc['thresholds'] < 0.5].sample(50, replace = False),\n",
    "        roc[roc['thresholds'] >= 0.5].sample(50, replace = False)\n",
    "    ], axis = 0, ignore_index = True).sort_values('thresholds')\n",
    "    classification_metrics.log_roc_curve(roc['fpr'].tolist(), roc['tpr'].tolist(), roc['thresholds'].tolist())\n",
    "\n",
    "    # Deployment evaluation.\n",
    "    return ('yes' if model_precision > baseline_precision else 'no',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFC (Random Forest Classifier) deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = ['google-cloud-aiplatform'],\n",
    "    base_image = 'python'\n",
    ")\n",
    "def rfc_deployment(project_id: str, region: str, model: Input[Model], serving_container_image_uri: str, vertex_model: Output[Model]):\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    # Authentication.\n",
    "    aiplatform.init(project = project_id, location = region)\n",
    "\n",
    "    DISPLAY_NAME = 'e-commerce_analysis'\n",
    "    MODEL_NAME = 'e-commerce_analysis-rfc'\n",
    "    ENDPOINT_NAME = 'e-commerce_analysis_endpoint'\n",
    "\n",
    "    def create_endpoint():\n",
    "        endpoints = aiplatform.Endpoint.list(\n",
    "            project = project_id,\n",
    "            location = region,\n",
    "            filter = f'display_name={ENDPOINT_NAME}',\n",
    "            order_by = 'create_time desc'\n",
    "        )\n",
    "        \n",
    "        if len(endpoints):\n",
    "            return endpoints[0] # Most recent.\n",
    "        else:\n",
    "            endpoint = aiplatform.Endpoint.create(\n",
    "                display_name = ENDPOINT_NAME,\n",
    "                project = project_id,\n",
    "                location = region\n",
    "            )\n",
    "            return endpoint\n",
    "    endpoint = create_endpoint()\n",
    "\n",
    "    # Input Artifact (Model).\n",
    "    model_upload = aiplatform.Model.upload(\n",
    "        display_name = DISPLAY_NAME,\n",
    "        artifact_uri = model.uri.replace('model', ''), # Directory path (with model.joblib/pkl).\n",
    "        serving_container_image_uri =  serving_container_image_uri,\n",
    "        serving_container_health_route = f'/v1/models/{MODEL_NAME}',\n",
    "        serving_container_predict_route = f'/v1/models/{MODEL_NAME}:predict',\n",
    "        serving_container_environment_variables = {\n",
    "            'MODEL_NAME': MODEL_NAME,\n",
    "        },\n",
    "    )\n",
    "    model_deploy = model_upload.deploy(\n",
    "        machine_type = 'e2-standard-2',\n",
    "        endpoint = endpoint,\n",
    "        traffic_split = {\n",
    "            '0': 100\n",
    "        },\n",
    "        deployed_model_display_name = DISPLAY_NAME\n",
    "    )\n",
    "\n",
    "    # Output Artifact (Model)\n",
    "    vertex_model.uri = model_deploy.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build the Pipeline with Components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "@dsl.pipeline(name = 'e-commerce_analysis')\n",
    "def ecommerce_analysis():\n",
    "    data_preprocessing_task = data_preprocessing(\n",
    "        project_id = PROJECT_ID,\n",
    "        bucket_name = BUCKET_NAME,\n",
    "        script_path = 'component_scripts/data_preprocessing.py',\n",
    "        data_gcs_uri = f'{GCS_BUCKET_URI}/data/data.csv',\n",
    "        product_details_gcs_uri = f'{GCS_BUCKET_URI}/data/product_details.xlsx'\n",
    "    )\n",
    "    overall_rfm_metrics_task = overall_rfm_metrics(\n",
    "        processed_data = data_preprocessing_task.outputs['processed_data']\n",
    "    )\n",
    "    rfm_scaling_task = rfm_scaling(\n",
    "        rfm = overall_rfm_metrics_task.outputs['rfm']\n",
    "    )\n",
    "    kmeans_training_and_prediction_task = kmeans_training_and_prediction(\n",
    "        rfm_scaled = rfm_scaling_task.outputs['rfm_scaled'],\n",
    "        n_clusters = 4\n",
    "    )\n",
    "    customer_churn_data_task = customer_churn_data(\n",
    "        processed_data = data_preprocessing_task.outputs['processed_data']\n",
    "    )\n",
    "    train_test_split_task = train_test_split(\n",
    "        rfm_churn = customer_churn_data_task.outputs['rfm_churn']\n",
    "    )\n",
    "    rfc_training_task = rfc_training(\n",
    "        train_data = train_test_split_task.outputs['train_data'],\n",
    "        hyperparams = {\n",
    "            'n_estimators': 50,\n",
    "            'min_samples_split': 4,\n",
    "            'min_samples_leaf': 2\n",
    "        }\n",
    "    )\n",
    "    rfc_evaluation_task = rfc_evaluation(\n",
    "        model = rfc_training_task.outputs['model'],\n",
    "        test_data = train_test_split_task.outputs['test_data'],\n",
    "        baseline_precision = 0.75\n",
    "    )\n",
    "\n",
    "    # Conditional deployment.\n",
    "    with dsl.Condition(\n",
    "        rfc_evaluation_task.outputs['deployment'] == 'yes',\n",
    "        name = 'rfc_quality'\n",
    "    ):\n",
    "        rfc_deployment_task = rfc_deployment(\n",
    "            project_id = PROJECT_ID,\n",
    "            region = REGION,\n",
    "            model = rfc_training_task.outputs['model'],\n",
    "            serving_container_image_uri = '',\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compile and Run the Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# The compiler generates a JSON file containing the pipeline structure, and the PipelineJob uses that JSON file to run the pipeline on GCP.\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func = ecommerce_analysis,\n",
    "    package_path = 'kubeflow_pipeline/ecommerce_analysis.json'\n",
    ")\n",
    "\n",
    "# The pipeline_root is the GCS location where pipeline artifacts (logs, metadata, and output data) are stored during and after the pipeline run.\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name = 'e-commerce_analysis',\n",
    "    template_path = 'kubeflow_pipeline/ecommerce_analysis.json',\n",
    "    pipeline_root = PIPELINE_ROOT,\n",
    "    project = PROJECT_ID, # Project ID.\n",
    "    location = REGION, # Region.\n",
    "    enable_caching = False\n",
    ")\n",
    "pipeline_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Make Predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project = PROJECT_ID, location = REGION)\n",
    "\n",
    "def endpoint_predict(endpoint_id: str, instances: list):\n",
    "    endpoint = aiplatform.Endpoint(endpoint_id)\n",
    "    predictions = endpoint.predict(instances = instances)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_id = '' # Deployed Model Endpoint.\n",
    "instances = [\n",
    "    [3,12,1569.45,3],\n",
    "    [29,1,316.61,2],\n",
    "    [26,3,154.58,1],\n",
    "    [7,1,115.1,1]\n",
    "]\n",
    "\n",
    "predictions = endpoint_predict(endpoint_id, instances)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
